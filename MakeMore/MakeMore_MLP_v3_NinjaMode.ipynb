{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rToK0Tku8PPn"
      },
      "source": [
        "## makemore: becoming a backprop ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ChBbac4y8PPq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "klmu3ZG08PPr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32616\n",
            "15\n",
            "['yama', 'yami', 'yamuna', 'yash', 'yashoda', 'yuda', 'yudhishthira', 'yudhisthira']\n"
          ]
        }
      ],
      "source": [
        "# read in all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "print(len(words))\n",
        "print(max(len(w) for w in words))\n",
        "print(words[-8:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BCQomLE_8PPs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ],
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V_zt2QHr8PPs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([185981, 3]) torch.Size([185981])\n",
            "torch.Size([23083, 3]) torch.Size([23083])\n",
            "torch.Size([23267, 3]) torch.Size([23267])\n"
          ]
        }
      ],
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eg20-vsg8PPt"
      },
      "outputs": [],
      "source": [
        "# ok boilerplate done, now we get to the action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MJPU8HT08PPu"
      },
      "outputs": [],
      "source": [
        "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZlFLjQyT8PPu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4137\n"
          ]
        }
      ],
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "# Note: I am initializating many of these parameters in non-standard ways\n",
        "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
        "# implementation of the backward pass.\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QY-y96Y48PPv"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "n = batch_size # a shorter variable also, for convenience\n",
        "# construct a minibatch\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8ofj1s6d8PPv"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(3.3327, grad_fn=<NegBackward0>)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
        "\n",
        "emb = C[Xb] # embed the characters into vectors\n",
        "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "# Linear layer 1\n",
        "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "# BatchNorm layer\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "bndiff = hprebn - bnmeani\n",
        "bndiff2 = bndiff**2\n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "bnraw = bndiff * bnvar_inv\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2 # output layer\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 27]), torch.Size([32, 1]))"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "counts.shape, counts_sum.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# loss = -(a+b+c)/3\n",
        "# dloss/da = -1/3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x1f45129ea70>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 64]),\n",
              " torch.Size([1, 64]),\n",
              " torch.Size([32, 64]),\n",
              " torch.Size([1, 64]))"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hpreact.shape, bngain.shape, bnraw.shape, bnbias.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "mO-8aqxK8PPw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1: backprop through the whole thing manually,\n",
        "# backpropagating through exactly all of the variables\n",
        "# as they are defined in the forward pass above, one by one\n",
        "\n",
        "dlogprobs = torch.zeros_like(logprobs)\n",
        "dlogprobs[range(n), Yb] = -1/n\n",
        "dprobs = (1.0 / probs) * dlogprobs\n",
        "dcounts_sum_inv = (dprobs * counts).sum(1, keepdims=True)\n",
        "dcounts = counts_sum_inv * dprobs\n",
        "dcounts_sum = -1 * counts_sum**-2 * dcounts_sum_inv\n",
        "dcounts += torch.ones_like(counts) * dcounts_sum\n",
        "dnorm_logits = (counts* dcounts)\n",
        "dlogits = dnorm_logits.clone()\n",
        "dlogit_maxes = (-dnorm_logits).sum(1, keepdims=True)\n",
        "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) *dlogit_maxes \n",
        "dh = dlogits @ W2.T\n",
        "dW2 = h.T @ dlogits\n",
        "db2 = dlogits.sum(0)\n",
        "dhpreact = (1 - h**2) * dh\n",
        "dbngain = (bnraw * dhpreact).sum(0, keepdims=True)\n",
        "dbnbias = dhpreact.sum(0, keepdims=True)\n",
        "dbnraw = bngain * dhpreact\n",
        "dbndiff = dbnraw * bnvar_inv\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdims=True)\n",
        "dbnvar = (-0.5 * (bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "dbndiff2 = (1.0/(n-1)) * torch.ones_like(bndiff2) * dbnvar\n",
        "dbndiff += 2 * bndiff * dbndiff2\n",
        "dhprebn = dbndiff.clone()\n",
        "dbnmeani = -dbndiff.sum(0, keepdims=True)\n",
        "dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "dembcat = dhprebn @ W1.T\n",
        "dW1 = embcat.T @ dhprebn\n",
        "db1 = dhprebn.sum(0)\n",
        "demb = dembcat.view(emb.shape)\n",
        "dC = torch.zeros_like(C)\n",
        "for i in range(Xb.shape[0]):\n",
        "    for j in range(Xb.shape[1]):\n",
        "        dC[Xb[i, j]] += demb[i, j]\n",
        "\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "cmp('probs', dprobs, probs)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "cmp('counts', dcounts, counts) \n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "cmp('logits', dlogits, logits)\n",
        "cmp('h', dh, h)\n",
        "cmp('W2', dW2, W2)\n",
        "cmp('b2', db2, b2)\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "cmp('embcat', dembcat, embcat)\n",
        "cmp('W1', dW1, W1)\n",
        "cmp('b1', db1, b1)\n",
        "cmp('emb', demb, emb)\n",
        "cmp('C', dC, C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "ebLtYji_8PPw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.332677125930786 diff: 4.76837158203125e-07\n"
          ]
        }
      ],
      "source": [
        "# Exercise 2: backprop through cross_entropy but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the loss,\n",
        "# take the derivative, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# logit_maxes = logits.max(1, keepdim=True).values\n",
        "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "# counts = norm_logits.exp()\n",
        "# counts_sum = counts.sum(1, keepdims=True)\n",
        "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "# probs = counts * counts_sum_inv\n",
        "# logprobs = probs.log()\n",
        "# loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# now:\n",
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "-gCXbB4C8PPx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logits          | exact: False | approximate: True  | maxdiff: 5.122274160385132e-09\n"
          ]
        }
      ],
      "source": [
        "# backward pass\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "dlogits = None # TODO. my solution is 3 lines\n",
        "dlogits = probs.clone()\n",
        "dlogits[range(n), Yb] -= 1\n",
        "dlogits /= n\n",
        "# -----------------\n",
        "\n",
        "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x1f45230d390>"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAKTCAYAAADlpSlWAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAALFBJREFUeJzt3XmsXOV5P/B3Zu7qlZjFS2wWsydgqrCLQEmgOERCEFAFSaSaCIFIARWslMhVgKCmcptKDU3lwD8pNFKAhCqAQC0RIcE0Ck4KKXKwwLFdUxuBoRC83Ou7zsxP50j2z7esth/7Hr/385EO13NneOadc95z5nvP8p5au91uJwCATNTHuwEAAJGEGwAgK8INAJAV4QYAyIpwAwBkRbgBALIi3AAAWelIFdNqtdJrr72Wpk6dmmq12ng3BwCogGJYvm3btqU5c+aker1+YIWbItjMmzdvvJsBAFTQxo0b09y5cw+scFPssSn8x3/8R5oyZcpe14scgLnRaKRI06ZNC6u1devW0L1nkbq6uiq5DE477bSwWs8880yaCD7sr6Xx6v+FzZs3h9Zj9xx66KGV3J4NDw+nqq4Dkd9Pkdvtnp6eFGloaCikTl9fXzr33HN35oQDKtzsOBRVBJuP8gH25wLv6IidXZEb96quJFUON5GHPSOC+L4S+TkjN+wR6/euRkdHU+6iD9VHbjequj0TbsY/3ER+B3zU9cAJxQBAVoQbACArwg0AkJV9Fm6WLVuWjjzyyPLY3Zlnnpl+85vf7Ku3AgDYt+HmRz/6UVq8eHG644470m9/+9t0yimnpIULF6Y333xzX7wdAMC+DTf/8A//kK699tr0la98JX3iE59I99xzT5o0aVL653/+533xdgAA+y7cFJfdPf/88+nCCy/8/29Sr5ePn3322fe8/r0Y02DXCQCgMuHmrbfeSs1mM82cOXPM74vHmzZtetfrly5dmqZPn75zMjoxAHBAXy21ZMmStGXLlp1TMawyAMCeCh+h+JBDDilHkX3jjTfG/L54PGvWrHe9vru7u5wAACq556YYZvnUU09NTz311JhhoYvHZ599dvTbAQDs+3tLFZeBL1q0qLwx4RlnnJHuuuuu1N/fX149BQBwwIWbK6+8Mv3v//5vuv3228uTiP/oj/4oPfHEE+86yRgAINo+uyv4jTfeWE4AABPqaikAgEjCDQCQlX12WGpvdXZ2ltPe6u3tTVEGBgZSVRUDJ0Ze8RY9anUV2/Zf//VfYbWOPfbYFGn9+vVhtUZHR8NqFaONRynGtYoUsb3Y9QrPKtaq1WopUuTy3Lx5c5oIItenyZMnh9UqRvuPMjIykiIVw8Ps7zr23AAAWRFuAICsCDcAQFaEGwAgK8INAJAV4QYAyIpwAwBkRbgBALIi3AAAWRFuAICsCDcAQFaEGwAgK8INAJAV4QYAyIpwAwBkRbgBALIi3AAAWRFuAICsdKTMHXbYYWG1/ud//idF+sMf/hBWa8qUKWG1tm3bliJ1dXWF1RodHQ2r1dvbG1Zr7dq1KdLIyEhYrenTp1eyb0T22UJfX19YrY6OjkrWarVaqara7faE+Jzd3d1htQYGBsJq1Wq1VFUjQduz3aljzw0AkBXhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGSlI1VUq9Uqp731yiuvpCjtdjtF6u7uDqvV19cXVqurqytFiliOOzQajbBao6OjYbWazWaK1NERt2pu27YtrNaJJ54YVuvVV19NVZ1nnZ2dYbX6+/vDatXr9cqum729vWG1Jk2aFFZr8+bNKdLw8HAlt2eR26BarZYiRfWN3dlm23MDAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGRFuAEAstKRKqrRaJTT3mq32ynK8PBwitTV1RVWq16Py6kR831fabVaYbWazWZYreOPPz5FWrNmTaqil19+OaxWX19fWK3C6OhoJftGT09PZbdBVV3XI/tGR0dlv+bSQQcdVMlt4zvvvJMiDQ0N7fc69twAAFkRbgCArAg3AEBWhBsAICvCDQCQFeEGAMiKcAMAZEW4AQCyItwAAFkRbgCArAg3AEBWhBsAICvCDQCQFeEGAMiKcAMAZEW4AQCyItwAAFkRbgCArHSkihoaGkpdXV2pSnp6ekLrDQ4OhtXq7u6uZLui51uz2azkPHvllVdSpOHh4bBajUYjVVG73Q6tV6vVwmqddNJJYbXWrl1b2WUZOc8mQp8t9Pb2htV66623KjnPGhWd/7vTLntuAICsCDcAQFaEGwAgK8INAJAV4QYAyEp4uPnmN79ZnoG/63TCCSdEvw0AwP67FPyTn/xk+tnPfvb/36SjslecAwCZ2Sepowgzs2bN2helAQD2/zk3a9asSXPmzEnz589PX/7yl9OGDRs+cLC+rVu3jpkAACoTbs4888x03333pSeeeCLdfffdaf369encc89N27Zte8/XL126NE2fPn3nNG/evOgmAQATSHi4ufjii9Of/umfpgULFqSFCxemf/u3f0ubN29OP/7xj9/z9UuWLElbtmzZOW3cuDG6SQDABLLPz/Q96KCD0nHHHfe+91cp7u8TeY8fAGBi2+fj3PT19aV169al2bNn7+u3AgCIDzdf+9rX0vLly8u7JP/qV79KX/jCF8o7eX7xi1+MfisAgH1/WOrVV18tg8zbb7+dDj300PTpT386rVixovw3AMABF24efPDB6JIAAB+Ze0sBAFkRbgCArFT2pk9Tpkwpp701MjKSorRarRSpONE6yujoaFitSZMmpUjbt28PqxU5bEAxOnYVl2V0veLmtVUU3a7Ier/73e/CakVug7q6ulKk4eHhsFqRN0h+6aWXKtvP+vv7K7me1+tx+yra7XaK1Gw293sde24AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArHamiBgYGUkfH3jdv/vz5Kcq6detSpFqtFlart7c3rNbo6GiKFLEcdxgaGqrk/G+322G1ouu1Wq2wWvV63N9D3d3dKVLk8ozUbDbDavX396dIkcvgpZdeqmQ/GxkZSZEajUZYrc7OzkpuZ4eHh1MV183dqWPPDQCQFeEGAMiKcAMAZEW4AQCyItwAAFkRbgCArAg3AEBWhBsAICvCDQCQFeEGAMiKcAMAZEW4AQCyItwAAFkRbgCArAg3AEBWhBsAICvCDQCQFeEGAMhKR6qoZrNZTntr/fr1KUq73Q6rFV2vv78/rFatVkuROjs7w2o1Go1K1po7d26K9Pvf/z6sVr1er2zfiDQwMBBWq6urK6zW0NBQJdsVvQ3q7u4OqzV58uSwWm+99VaKFLndiPiO22F0dLSy33XjwZ4bACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArwg0AkJWOlLmjjjoqrNbGjRtTpMHBwbBa3d3dYbWGhoZSpFqtFlar2WxWsl2vvPJKijQ8PFzJz9lutyvZ/wutVitVUW9vbyX7RXTfiGxb5Hre1dWVqro8t2zZElar0WiE1arXq7nfY3faVc1PAACwh4QbACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArHamiPvWpT6VarbbXddauXZuiDA0NpUj1ejWzZaPRSFXV29tbyeUZ3Tc6OzvDajWbzVRF0f0/st92dMRtGvv6+sJqRWwTd9Vut8NqTZkyJazW6OhoWK2RkZEUqb+/v5LreavVqmStwuTJk9P+7hfV/HYFANhDwg0AkBXhBgDIinADAGRFuAEAsiLcAAATO9w888wz6ZJLLklz5swpL0t85JFH3nVp4e23355mz55dXrZ74YUXpjVr1kS2GQAgLtwU1/ifcsopadmyZe/5/Le//e303e9+N91zzz3p17/+dXl9+8KFC9Pg4ODuvhUAwG7b7ZGqLr744nJ6L8Vem7vuuit94xvfSJdeemn5ux/84Adp5syZ5R6eq6666j0HP9t1ALStW7fubpMAAPbNOTfr169PmzZtKg9F7TB9+vR05plnpmefffY9/5+lS5eWr9kxzZs3L7JJAMAEExpuimBTKPbU7Kp4vOO5/2vJkiVpy5YtO6eNGzdGNgkAmGDG/d5S3d3d5QQAULk9N7NmzSp/vvHGG2N+Xzze8RwAwAETbo466qgyxDz11FNjThAurpo6++yzI98KACDmsFRfX19au3btmJOIX3jhhTRjxox0+OGHp5tvvjl961vfSscee2wZdm677bZyTJzLLrtsd98KAGDfh5vnnnsufeYzn9n5ePHixeXPRYsWpfvuuy/deuut5Vg41113Xdq8eXP69Kc/nZ544onU09Oz+60DANhNtXYxOE2FFIexikvCC8UIyHuryqMjR3y+HTo7O8NqjY6OpkiRbevq6gqrtev4SnsrepDKyHnWbDZTFdXr1b37SzG6epRib3cVtxmFyM3/lClTKrkNGhkZSVUVuTxbrVZYrXZwLCgG842wbdu2dNxxx5VXVk+bNu0DX1vdrQsAwB4QbgCArIz7ODfvZ/Xq1Wnq1Kl7XWd4eDhVdfd+5G75yMMiCxYsSJFWrlxZyV3Mkcuz0WikSJHnqG3fvj2s1qRJkyp5uKbQ0dFRycMikbv4o89djFyfIvtZ5OGa6MOfkfUGBgYq2f87AmtF9rPdqWPPDQCQFeEGAMiKcAMAZEW4AQCyItwAAFkRbgCArAg3AEBWhBsAICvCDQCQFeEGAMiKcAMAZEW4AQCyItwAAFkRbgCArAg3AEBWhBsAICvCDQCQFeEGAMhKR6qooaGh1NXVtdd15s+fn6KsWbMmRWo2m2G12u12WK3+/v4Uad26dZWcZ52dnWG1enp6UqTIZdDREbea1+vV/XtoZGQkrNbw8HAl51mxXYzU3d0dVmtgYGBC9NnIZRDxHbcv5lkzcDtbGBwc3O/zvrpbKgCAPSDcAABZEW4AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZ6UgVdfrpp6darbbXdf7whz+kKG+99VaKVK/XK1nrnXfeSZE6OuK6WUSf2KHZbIbVGhwcTJEajUZYrXa7HVZr+/btYbVarVaqaj+L7BtVbVd0vchtUGTfiO5nkZ8zsm9Eagb3s6h5tjvbf3tuAICsCDcAQFaEGwAgK8INAJAV4QYAyIpwAwBkRbgBALIi3AAAWRFuAICsCDcAQFaEGwAgK8INAJAV4QYAyIpwAwBkRbgBALIi3AAAWRFuAICsCDcAQFY6UkWtXLkyTZ06da/rbNu2LUVpNBopUrvdDqtVq9XCavX19aVIrVYrrNa0adMq+TlHRkZSpM7OzrBao6OjaSKI7GfHH398WK21a9eG1arXY/8eHR4eDqvV09MTVmtoaCis1uTJk1OkLVu2VPI7JfI7YHBwMEXq6ura79tFe24AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArHami+vv7U72+99mr0WikKCMjIynSpEmTwmoNDAyE1eru7k6ROjs7w2oNDQ1V8nMW/TXS6OhoWK12u13JZRmxfu+refb73/8+rFaz2Qyr1dERu8mOXAbz5s0LqxW53V6zZk2KFLk+RfbZKhseHt7vdey5AQCyItwAAFkRbgCArAg3AEBWhBsAYGKHm2eeeSZdcsklac6cOalWq6VHHnlkzPNXX311+ftdp8997nORbQYAiAs3xSWvp5xySlq2bNn7vqYIM6+//vrO6YEHHtjdtwEA2CO7PWjCxRdfXE4fNn7IrFmz9qxFAABVO+fm6aefTocddlg6/vjj01e/+tX09ttvf+CgbFu3bh0zAQBUJtwUh6R+8IMfpKeeeir93d/9XVq+fHm5p+f9RulcunRpmj59+s4pcpRLAGDiCb/9wlVXXbXz3yeffHJasGBBOvroo8u9ORdccMG7Xr9kyZK0ePHinY+LPTcCDgBQ2UvB58+fnw455JC0du3a9z0/Z9q0aWMmAIDKhptXX321POdm9uzZ+/qtAAB2/7BUX1/fmL0w69evTy+88EKaMWNGOd15553piiuuKK+WWrduXbr11lvTMccckxYuXBjddgCAvQ83zz33XPrMZz6z8/GO82UWLVqU7r777rRy5cr0L//yL2nz5s3lQH8XXXRR+uu//uvy8BMAQOXCzfnnn5/a7fb7Pv/Tn/50b9sEALDH3FsKAMiKcAMAZCV8nJson/rUp0LqbNiwIUVptVop0uDgYFit4galUer12MwbOd+il0FV51lVl2dkrdHR0RSps7OzkvN/ZGQkVVWj0Qi9MjZKcTFKlA86jWJPdHR0VHIdiGxXPXh7FtW2rq6uj/xae24AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVjpSRR1yyCGpXt/77DU0NJSiNBqNVFWtVius1uDgYIrUbDbDak2ZMiWs1rZt21JV1Wq1VEWR61O73U5VXQci1/Xu7u5Kfsbo5Rkp8nNGbjOitxtdXV2V3M62gvtZ1PZsd+rYcwMAZEW4AQCyItwAAFkRbgCArAg3AEBWhBsAICvCDQCQFeEGAMiKcAMAZEW4AQCyItwAAFkRbgCArAg3AEBWhBsAICvCDQCQFeEGAMiKcAMAZEW4AQCy0pEq6le/+lWaOnXqXtcZGhoKac9E0tXVFVqvr68vrNbWrVvDarXb7bBa9Xrs3wm9vb2V/JxHH310WK1Vq1alSJGfs9VqhdVqNpthtUZGRlKk7u7usFrDw8OV3Ab19/enSLVaLVVRZJ9tNBop0ujo6H6vY88NAJAV4QYAyIpwAwBkRbgBALIi3AAAWRFuAICsCDcAQFaEGwAgK8INAJAV4QYAyIpwAwBkRbgBALIi3AAAWRFuAICsCDcAQFaEGwAgK8INAJAV4QYAyIpwAwBkpSNVVF9fX6rVantd59hjj01RNm7cmCINDg6G1eru7g6d95F6enrCajWbzbBajUYjrFar1UqRIpdB5OdctWpVWK12u50iRS6DyD5br0+MvyEj+1mk6Pnf29sbVmvLli3Zz//xMjHWOgBgwhBuAICsCDcAQFaEGwAgK8INAJAV4QYAyIpwAwBkRbgBALIi3AAAWRFuAICsCDcAQFaEGwAgK8INAJAV4QYAyIpwAwBkRbgBALIi3AAAWRFuAICsdKSKqtVq5bS31qxZk6oq4vPti1rR6vW4DN1ut8NqdXTEdf+BgYEUKXJ5NhqNsFojIyOVnP/RfePYY48Nq/Xyyy+H1Wq1WqmqJk2aFFZrcHCwkv0/um0TRTto3dydOvbcAABZEW4AgKwINwBAVoQbACArwg0AMHHDzdKlS9Ppp5+epk6dmg477LB02WWXpdWrV7/rTPIbbrghHXzwwWnKlCnpiiuuSG+88UZ0uwEA9j7cLF++vAwuK1asSE8++WR5WehFF12U+vv7d77mlltuSY899lh66KGHyte/9tpr6fLLL9+dtwEA2GO7NdDEE088MebxfffdV+7Bef7559N5552XtmzZkr7//e+n+++/P332s58tX3PvvfemE088sQxEZ5111p63FABgX59zU4SZwowZM8qfRcgp9uZceOGFO19zwgknpMMPPzw9++yz71ljaGgobd26dcwEALDfw00xUubNN9+czjnnnHTSSSeVv9u0aVPq6upKBx100JjXzpw5s3zu/c7jmT59+s5p3rx5e9okAIA9DzfFuTcvvvhievDBB/eqAUuWLCn3AO2YNm7cuFf1AICJbY9u7nLjjTemxx9/PD3zzDNp7ty5O38/a9asNDw8nDZv3jxm701xtVTx3Hvp7u4uJwCA/b7nprhpVRFsHn744fTzn/88HXXUUWOeP/XUU1NnZ2d66qmndv6uuFR8w4YN6eyzzw5pMABA2J6b4lBUcSXUo48+Wo51s+M8muJcmd7e3vLnNddckxYvXlyeZDxt2rR00003lcHGlVIAQOXCzd13313+PP/888f8vrjc++qrry7//Z3vfCfV6/Vy8L7iSqiFCxem733ve5FtBgCICTfFYakP09PTk5YtW1ZOAAD7m3tLAQBZEW4AgKzs0aXg+0Oj0SinvVUMKhhl+/btKVJV21bc9DRSMWp1FWuNjo6mquroiFs1P8rh5I+quBoySnFOXqTiXL8oK1eurGSfrdVqqarzbNd7DFapz0Z+xkKz2QyrFfEdty++T4477rgUqRgTb3+z5wYAyIpwAwBkRbgBALIi3AAAWRFuAICsCDcAQFaEGwAgK8INAJAV4QYAyIpwAwBkRbgBALIi3AAAWRFuAICsCDcAQFaEGwAgK8INAJAV4QYAyIpwAwBkpSNV1OjoaDlF1InS09OTIg0ODobV6u3tDav1zjvvpEjR862KGo1GaL3h4eHKtq2q7Wq1WpVcn+r1eiX7RaFWq4XVarfbYbVOPPHEsFpr1qxJkSZPnhxWa8uWLZXsG6tWrUpVtDv91Z4bACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArwg0AkBXhBgDISkeqqO7u7nLaW4ODgyHtia5ViPh8O7Tb7bBajUYjRRoaGgqr1dER12WPOeaYsFqrVq1KkXp6esJqjYyMVHL+Dw8Pp6quT52dnWG1tm3bFlarXo/9e7RWq4XV6urqCqv10ksvhdVqtVopUuTyjNzWRi7LzsD+H/n9tDvbH3tuAICsCDcAQFaEGwAgK8INAJAV4QYAyIpwAwBkRbgBALIi3AAAWRFuAICsCDcAQFaEGwAgK8INAJAV4QYAyIpwAwBkRbgBALIi3AAAWRFuAICsCDcAQFY6UkUNDw+XU5XU67FZcGhoqJJt6+rqSpGazWZYrZGRkbBaq1evDqvVbrdTVftGd3d3WK3Ozs5K9otC5PZi/vz5YbXWrFlT2Xk2Y8aMsFpvv/12WK2Ojo5KbjOqvK2NnGcDAwMp0ujo6H7fLtpzAwBkRbgBALIi3AAAWRFuAICsCDcAQFaEGwAgK8INAJAV4QYAyIpwAwBkRbgBALIi3AAAWRFuAICsCDcAQFaEGwAgK8INAJAV4QYAyIpwAwBkRbgBALLSkSqqq6urnHI2OjqaqqjValX2c9br9Uq2K7qvdnTErZqDg4NhtYaHh1NVdXd3h9Vau3ZtWK1msxlWq9FopEhvv/12JdfNSNHtarfbqYoit2fHHHNMirRu3br9vl2sZm8EANhDwg0AkBXhBgDIinADAGRFuAEAJm64Wbp0aTr99NPT1KlT02GHHZYuu+yytHr16jGvOf/881OtVhszXX/99dHtBgDY+3CzfPnydMMNN6QVK1akJ598Mo2MjKSLLroo9ff3j3ndtddem15//fWd07e//e3deRsAgD22W4NpPPHEE2Me33fffeUenOeffz6dd955O38/adKkNGvWrD1vFQDAeJxzs2XLlvLnjBkzxvz+hz/8YTrkkEPSSSedlJYsWZK2b9/+vjWGhobS1q1bx0wAAHuqY29Gsb355pvTOeecU4aYHb70pS+lI444Is2ZMyetXLkyff3rXy/Py/nJT37yvufx3HnnnXvaDACAmHBTnHvz4osvpl/+8pdjfn/dddft/PfJJ5+cZs+enS644IJy+OWjjz76XXWKPTuLFy/e+bjYczNv3rw9bRYAMMHtUbi58cYb0+OPP56eeeaZNHfu3A987ZlnnrnzXi3vFW6K+8FE3hMGAJjYOnb3hmE33XRTevjhh9PTTz+djjrqqA/9f1544YXyZ7EHBwCgUuGmOBR1//33p0cffbQc62bTpk3l76dPn556e3vLQ0/F85///OfTwQcfXJ5zc8stt5RXUi1YsGBffQYAgD0LN3fffffOgfp2de+996arr746dXV1pZ/97GfprrvuKse+Kc6dueKKK9I3vvGN3XkbAID9d1jqgxRhphjoDwBgvLi3FACQFeEGAMjKHo9zs68Vh8A+7DDYR9FsNlOUiPbsqqMjbva/12X2e2rNmjUpUnEuVhXn2cDAQFitYqTtSJ2dnWG16vWJ8TdM5PKMnGeRfXZ0dDRFajQaYbWKi0yi9PX1VXL7UxgeHk5VVNzrMcrLL7+cIkV9D+9OnYmx1QMAJgzhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGSlI1VUq9Uqp71Vr8flt8hahXa7HVZr/fr1YbU6OztTpL6+vrBaw8PDYbUi+tcOtVotVbVvRPfbqrYrehlUcX1qNpspUkdH3FfAxz/+8bBaq1atCqs1MjKSqjrPItsW2a5oUf12d7aL1dzqAQDsIeEGAMiKcAMAZEW4AQCyItwAAFkRbgCArAg3AEBWhBsAICvCDQCQFeEGAMiKcAMAZEW4AQCyItwAAFkRbgCArAg3AEBWhBsAICvCDQCQFeEGAMhKR6qoer1eTntreHg4RWk0GilSs9kMq/Wxj30srNahhx6aIr3yyithtVqtVqqi6L7RbrfDatVqtUr22cjPGL0MIrY9+0J0/x8ZGQmrtWrVqkr2jehlOXXq1LBa77zzzoTo/62gfrs727JqrsEAAHtIuAEAsiLcAABZEW4AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGRFuAEAstKRKqrdbpfT3po0aVKKMjAwkCLV63HZcvv27WG11q9fnyK1Wq1K1mo2m2G1arVaijRlypSwWtu2bQur1d3dHVZraGgoRZo8eXIl1/XIzxndz6LrRWk0GpX9jP39/ZVsW8T35Q7HHXdcirR69er93i/suQEAsiLcAABZEW4AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZ6UgVNTIyUk4RdaL09PSkSIODg6mKjjzyyNB6GzduDKvVbDbDanV1dYXVarVaKdLWrVvDajUajbBaketTrVZLVZ1nket6Z2dnWK3h4eEUKXIZRNaq16v7d3dk3xgaGqrktvHll19Okdrt9n6vU90eBACwB4QbACArwg0AkBXhBgDIinADAGRFuAEAJm64ufvuu9OCBQvStGnTyunss89O//7v/z7m0uYbbrghHXzwwWnKlCnpiiuuSG+88ca+aDcAwN6Hm7lz56a//du/Tc8//3x67rnn0mc/+9l06aWXplWrVpXP33LLLemxxx5LDz30UFq+fHl67bXX0uWXX747bwEAsP8G8bvkkkvGPP6bv/mbcm/OihUryuDz/e9/P91///1l6Cnce++96cQTTyyfP+uss953EKNdBzKKHIgLAJh46nszGuKDDz6Y+vv7y8NTxd6cYvTSCy+8cOdrTjjhhHT44YenZ5999n3rLF26NE2fPn3nNG/evD1tEgDA7oeb3/3ud+X5NN3d3en6669PDz/8cPrEJz6RNm3aVA5nf9BBB415/cyZM8vn3s+SJUvSli1bdk6RQ/UDABPPbt9b6vjjj08vvPBCGUT+9V//NS1atKg8v2ZPFSGpmAAAxiXcFHtnjjnmmPLfp556avrP//zP9I//+I/pyiuvLG/qtnnz5jF7b4qrpWbNmhXSWACAfT7OTXE35OKE4CLoFHe/feqpp3Y+t3r16rRhw4bynBwAgMrtuSnOj7n44ovLk4S3bdtWXhn19NNPp5/+9KflycDXXHNNWrx4cZoxY0Y5Ds5NN91UBpv3u1IKAGBcw82bb76Z/uzP/iy9/vrrZZgpBvQrgs2f/MmflM9/5zvfSfV6vRy8r9ibs3DhwvS9730vvNEAACHhphjH5oP09PSkZcuWlRMAwHhwbykAICvCDQAwsS8F31+KS86LKaJOlNHR0RSpOIwXpbhqLcp///d/p0jFDVWjFFfkVbFdtVotVVUxmniU4py6Kvb/wsDAQCXX9UajUcn1PFrk54ysFbmeRy+D0047LazWypUrK/td12639/u2zJ4bACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArwg0AkBXhBgDIinADAGRFuAEAsiLcAABZEW4AgKwINwBAVoQbACArHali2u12+bOvry+kXldXV4oyOjqaqqrVaoXVajabYbWi63V2dobVGhkZCatVq9XSvlgPqqZer1dy/hcGBgbCajUajUrWip5nkao6zwYHB1OkyLZFrufbtm2r7HddO+hz7sgFH6VerV2xreirr76a5s2bN97NAAAqaOPGjWnu3LkHVrgp9kC89tpraerUqR/41/DWrVvLEFR8yGnTpu3XNmL+jzfzf/xZBuPL/J9487/dbpd7qObMmfOhe5Erd1iqaPCHJbJdFTNVxx4/5v/4Mv/Hn2Uwvsz/iTX/p0+f/pFe54RiACArwg0AkJUDNtx0d3enO+64o/zJ/mf+jy/zf/xZBuPL/B9f3RWf/5U7oRgAYELuuQEAeC/CDQCQFeEGAMiKcAMAZEW4AQCyckCGm2XLlqUjjzwy9fT0pDPPPDP95je/Ge8mTRjf/OY3y9ti7DqdcMIJ492sbD3zzDPpkksuKYcbL+b1I488Mub54mLH22+/Pc2ePTv19vamCy+8MK1Zs2bc2jvR5v/VV1/9rvXhc5/73Li1NzdLly5Np59+enk7nsMOOyxddtllafXq1e+6MeYNN9yQDj744DRlypR0xRVXpDfeeGPc2jzR5v/555//rnXg+uuvT+PtgAs3P/rRj9LixYvL6+t/+9vfplNOOSUtXLgwvfnmm+PdtAnjk5/8ZHr99dd3Tr/85S/Hu0nZ6u/vL/t4Eejfy7e//e303e9+N91zzz3p17/+dZo8eXK5PkTfCXmi+rD5XyjCzK7rwwMPPLBf25iz5cuXl8FlxYoV6cknnyzvin7RRReVy2WHW265JT322GPpoYceKl9f3Jvw8ssvH9d2T6T5X7j22mvHrAPFdmnctQ8wZ5xxRvuGG27Y+bjZbLbnzJnTXrp06bi2a6K444472qeccsp4N2NCKlbXhx9+eOfjVqvVnjVrVvvv//7vd/5u8+bN7e7u7vYDDzwwTq2cOPO/sGjRovall146bm2aaN58881yOSxfvnxnf+/s7Gw/9NBDO1/z0ksvla959tlnx7GlE2P+F/74j/+4/Rd/8Rftqjmg9twMDw+n559/vtz1vuuNNovHzz777Li2bSIpDnsUu+nnz5+fvvzlL6cNGzaMd5MmpPXr16dNmzaNWR+Km8oVh2qtD/vP008/Xe6yP/7449NXv/rV9Pbbb493k7K1ZcuW8ueMGTPKn8X3QbE3Ydd1oDhMfvjhh1sH9sP83+GHP/xhOuSQQ9JJJ52UlixZkrZv357GW+XuCv5B3nrrrdRsNtPMmTPH/L54/PLLL49buyaS4ovzvvvuKzfkxe7HO++8M5177rnpxRdfLI/Lsv8UwabwXuvDjufYt4pDUsUhkKOOOiqtW7cu/dVf/VW6+OKLyy/WRqMx3s3LSqvVSjfffHM655xzyi/RQtHPu7q60kEHHTTmtdaB/TP/C1/60pfSEUccUf7Bu3LlyvT1r3+9PC/nJz/5SRpPB1S4YfwVG+4dFixYUIadomP/+Mc/Ttdcc824tg32t6uuumrnv08++eRynTj66KPLvTkXXHDBuLYtN8W5H8UfUc7xq9b8v+6668asA8XFDUXfL8J+sS6MlwPqsFSx26v4a+j/nglfPJ41a9a4tWsiK/5iOu6449LatWvHuykTzo4+b32ojuJQbbGdsj7EuvHGG9Pjjz+efvGLX6S5c+fu/H3Rz4vTFTZv3jzm9daB/TP/30vxB29hvNeBAyrcFLsfTz311PTUU0+N2VVWPD777LPHtW0TVV9fX5nQi7TO/lUcCik24LuuD1u3bi2vmrI+jI9XX321POfG+hCjOI+7+GJ9+OGH089//vOyz++q+D7o7Owcsw4Uh0SK8wCtA/t+/r+XF154ofw53uvAAXdYqrgMfNGiRem0005LZ5xxRrrrrrvKy9K+8pWvjHfTJoSvfe1r5bgfxaGo4pLL4pL8Ym/aF7/4xfFuWrbhcde/gIqTiIuNR3FCX3HSZHEM/Fvf+lY69thjyw3PbbfdVh77LsajYN/O/2IqzjkrxlUpQmYR8m+99dZ0zDHHlJfjE3Mo5P7770+PPvpoeU7fjvNoihPni3Gdip/F4fDie6FYHtOmTUs33XRTGWzOOuus8W5+9vN/3bp15fOf//zny3GGinNuikvzzzvvvPIQ7bhqH4D+6Z/+qX344Ye3u7q6ykvDV6xYMd5NmjCuvPLK9uzZs8t5//GPf7x8vHbt2vFuVrZ+8YtflJde/t+puAR5x+Xgt912W3vmzJnlJeAXXHBBe/Xq1ePd7Akx/7dv396+6KKL2oceemh5OfIRRxzRvvbaa9ubNm0a72Zn473mfTHde++9O18zMDDQ/vM///P2xz72sfakSZPaX/jCF9qvv/76uLZ7osz/DRs2tM8777z2jBkzyu3PMccc0/7Lv/zL9pYtW8a76e1a8Z/xjVcAABP0nBsAgA8j3AAAWRFuAICsCDcAQFaEGwAgK8INAJAV4QYAyIpwAwBkRbgBALIi3AAAWRFuAICUk/8HqHe8j4yiBPcAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(dlogits.detach(), cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "hd-MkhB68PPy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ],
      "source": [
        "# Exercise 3: backprop through batchnorm but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
        "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
        "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "# bndiff = hprebn - bnmeani\n",
        "# bndiff2 = bndiff**2\n",
        "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "# bnraw = bndiff * bnvar_inv\n",
        "# hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# now:\n",
        "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "POdeZSKT8PPy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ],
      "source": [
        "# backward pass\n",
        "\n",
        "# before we had:\n",
        "# dbnraw = bngain * dhpreact\n",
        "# dbndiff = bnvar_inv * dbnraw\n",
        "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "# dbndiff += (2*bndiff) * dbndiff2\n",
        "# dhprebn = dbndiff.clone()\n",
        "# dbnmeani = (-dbndiff).sum(0)\n",
        "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "\n",
        "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
        "# (you'll also need to use some of the variables from the forward pass up above)\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "dhprebn = None # TODO. my solution is 1 (long) line\n",
        "dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(bnraw*dhpreact).sum(0)) \n",
        "# -----------------\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "wPy8DhqB8PPz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12297\n"
          ]
        }
      ],
      "source": [
        "# Exercise 4: putting it all together!\n",
        "# Train the MLP neural net with your own backward pass\n",
        "\n",
        "# init\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "n = batch_size # convenience\n",
        "lossi = []\n",
        "\n",
        "# use this context manager for efficiency once your backward pass is written (TODO)\n",
        "with torch.no_grad():\n",
        "\n",
        "# kick off optimization\n",
        "  for i in range(max_steps):\n",
        "\n",
        "    # minibatch construct\n",
        "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "    # forward pass\n",
        "    emb = C[Xb] # embed the characters into vectors\n",
        "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "    # Linear layer\n",
        "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "    # BatchNorm layer\n",
        "    # -------------------------------------------------------------\n",
        "    bnmean = hprebn.mean(0, keepdim=True)\n",
        "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
        "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
        "    hpreact = bngain * bnraw + bnbias\n",
        "    # -------------------------------------------------------------\n",
        "    # Non-linearity\n",
        "    h = torch.tanh(hpreact) # hidden layer\n",
        "    logits = h @ W2 + b2 # output layer\n",
        "    loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "    # backward pass\n",
        "    for p in parameters:\n",
        "      p.grad = None\n",
        "    # loss.backward() # use this for correctness comparisons, delete it later!\n",
        "\n",
        "    # manual backprop! #swole_doge_meme\n",
        "    # -----------------\n",
        "    dlogits = F.softmax(logits, 1)\n",
        "    dlogits[range(n), Yb] -= 1\n",
        "    dlogits /= n\n",
        "    dh = dlogits @ W2.T\n",
        "    dW2 = h.T @ dlogits\n",
        "    db2 = dlogits.sum(0)\n",
        "    dhpreact = (1 - h**2) * dh\n",
        "    dbngain = (bnraw * dhpreact).sum(0, keepdims=True)\n",
        "    dbnbias = dhpreact.sum(0, keepdims=True)\n",
        "    dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(bnraw*dhpreact).sum(0))\n",
        "\n",
        "    dembcat = dhprebn @ W1.T\n",
        "    dW1 = embcat.T @ dhprebn\n",
        "    db1 = dhprebn.sum(0)\n",
        "\n",
        "    demb = dembcat.view(emb.shape)\n",
        "    dC = torch.zeros_like(C)\n",
        "    for i in range(Xb.shape[0]):\n",
        "        for j in range(Xb.shape[1]):\n",
        "            dC[Xb[i, j]] += demb[i, j]\n",
        "    \n",
        "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
        "  \n",
        "    # -----------------\n",
        "\n",
        "    # update\n",
        "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "    for p, grad in zip(parameters, grads):\n",
        "      # p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
        "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
        "\n",
        "    # track stats\n",
        "    if i % 10000 == 0: # print every once in a while\n",
        "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "    lossi.append(loss.log10().item())\n",
        "\n",
        "    # if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
        "    #   break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "ZEpI0hMW8PPz"
      },
      "outputs": [],
      "source": [
        "# useful for checking your gradients\n",
        "# for p,g in zip(parameters, grads):\n",
        "#   cmp(str(tuple(p.shape)), g, p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "KImLWNoh8PP0"
      },
      "outputs": [],
      "source": [
        "# calibrate the batch norm at the end of training\n",
        "\n",
        "with torch.no_grad():\n",
        "  # pass the training set through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  # measure the mean/std over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "6aFnP_Zc8PP0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train 2.1327898502349854\n",
            "val 2.1718714237213135\n"
          ]
        }
      ],
      "source": [
        "# evaluate train and val loss\n",
        "\n",
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "esWqmhyj8PP1"
      },
      "outputs": [],
      "source": [
        "# I achieved:\n",
        "# train 2.0718822479248047\n",
        "# val 2.1162495613098145"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "xHeQNv3s8PP1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "montaymya.\n",
            "zielle.\n",
            "dhryal.\n",
            "temursteng.\n",
            "leo.\n",
            "adelynneli.\n",
            "jemi.\n",
            "jenleigh.\n",
            "van.\n",
            "aaraelyzion.\n",
            "kamin.\n",
            "sadhvrishiriel.\n",
            "kin.\n",
            "renley.\n",
            "xithana.\n",
            "ubdence.\n",
            "ryyah.\n",
            "fiel.\n",
            "yuma.\n",
            "mustella.\n"
          ]
        }
      ],
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "\n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # forward pass\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
        "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "      hpreact = embcat @ W1 + b1\n",
        "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "      logits = h @ W2 + b2 # (N, vocab_size)\n",
        "      # sample\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      if ix == 0:\n",
        "        break\n",
        "\n",
        "    print(''.join(itos[i] for i in out))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "karpathy",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
